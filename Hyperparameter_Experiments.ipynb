{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sMnEbmPFWvci",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMnEbmPFWvci",
        "outputId": "07174de9-2cc7-4c91-c0e4-ce5232fb9e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 4.37.2\n",
            "Uninstalling transformers-4.37.2:\n",
            "  Successfully uninstalled transformers-4.37.2\n",
            "Found existing installation: peft 0.10.0\n",
            "Uninstalling peft-0.10.0:\n",
            "  Successfully uninstalled peft-0.10.0\n"
          ]
        }
      ],
      "source": [
        "pip uninstall -y transformers peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I4HzJDnaWyB3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4HzJDnaWyB3",
        "outputId": "ec191fbf-7600-4805-96d7-f3e990644be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.37.2\n",
            "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "Collecting peft==0.10.0\n",
            "  Using cached peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (0.27.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (1.1.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.10.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (3.0.2)\n",
            "Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "Using cached peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "Installing collected packages: transformers, peft\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed peft-0.10.0 transformers-4.37.2\n"
          ]
        }
      ],
      "source": [
        "pip install transformers==4.37.2 peft==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EzUWqyBhWzfN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzUWqyBhWzfN",
        "outputId": "43a62ca8-72a7-4163-8b6c-f07f7dbd27b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate==0.27.2 in /usr/local/lib/python3.11/dist-packages (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.27.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.27.2) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.27.2) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.27.2) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.27.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from accelerate==0.27.2) (0.33.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.27.2) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.27.2) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->accelerate==0.27.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->accelerate==0.27.2) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->accelerate==0.27.2) (1.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->accelerate==0.27.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->accelerate==0.27.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->accelerate==0.27.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->accelerate==0.27.2) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "pip install accelerate==0.27.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y8FTiXW5W1ax",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8FTiXW5W1ax",
        "outputId": "201e1324-4201-498a-a854-1146718e13fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tTJQ6qiOW5Kp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTJQ6qiOW5Kp",
        "outputId": "9eecf575-2d81-4c21-cc5a-857cd0398338"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.37.2\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "print(transformers.__file__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8e99229",
      "metadata": {
        "id": "f8e99229"
      },
      "source": [
        "## ðŸ”¹ Imports and Hyperparameter Grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b700b41",
      "metadata": {
        "id": "9b700b41"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
        "                             confusion_matrix, ConfusionMatrixDisplay, classification_report,\n",
        "                             roc_curve, auc)\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments,\n",
        "                          Trainer, EarlyStoppingCallback, set_seed)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "hyperparameter_grid = [\n",
        "    {\"learning_rate\": 5e-5, \"weight_decay\": 0.01},\n",
        "    {\"learning_rate\": 3e-5, \"weight_decay\": 0.01},\n",
        "    {\"learning_rate\": 2e-5, \"weight_decay\": 0.1},\n",
        "    {\"learning_rate\": 2e-5, \"weight_decay\": 0.01},  # baseline\n",
        "    {\"learning_rate\": 1e-5, \"weight_decay\": 0.01},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e8c6a3",
      "metadata": {
        "id": "b6e8c6a3"
      },
      "source": [
        "## ðŸ”¹ Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a71ef2",
      "metadata": {
        "id": "03a71ef2"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_cache(emails, tokenizer_name=\"microsoft/deberta-v3-base\", max_length=320,\n",
        "                       save_path=\"tokenized_emails.pt\", force_retokenize=False):\n",
        "    if Path(save_path).exists() and not force_retokenize:\n",
        "        print(f\"Loading tokenized data from '{save_path}'...\")\n",
        "        return torch.load(save_path)\n",
        "    print(f\"Tokenizing {len(emails)} emails with max_length={max_length}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    tokenized = tokenizer(\n",
        "        emails, max_length=max_length, padding=\"max_length\", truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    torch.save(tokenized, save_path)\n",
        "    print(f\"Saved tokenized data to '{save_path}'\")\n",
        "    return tokenized\n",
        "\n",
        "def save_classification_report(report_str, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(report_str)\n",
        "    print(f\"Saved classification report to {filename}\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    probs = logits[:, 1] if logits.shape[1] > 1 else logits[:, 0]\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(labels, probs)\n",
        "    except ValueError:\n",
        "        roc_auc = float('nan')\n",
        "    return {\n",
        "        'accuracy': acc, 'f1': f1, 'precision': precision,\n",
        "        'recall': recall, 'roc_auc': roc_auc\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03199218",
      "metadata": {
        "id": "03199218"
      },
      "source": [
        "## ðŸ”¹ Load Dataset and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sWtCvb-YSSlt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWtCvb-YSSlt",
        "outputId": "f6254ab4-1c94-4949-aeb0-eb6dc277ce7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "experiment_index = 4  # replace with desired config index\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define a base path in Google Drive to save outputs\n",
        "data_path = '/content/drive/My Drive/Cybersecurity Practicum/'\n",
        "drive_base_path = '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results'\n",
        "Deberta_data_path = '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data'\n",
        "report_path = '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results'\n",
        "report_dir = os.path.join(report_path, f\"config_{experiment_index}\")\n",
        "import os\n",
        "os.makedirs(drive_base_path, exist_ok=True)\n",
        "os.makedirs(Deberta_data_path, exist_ok=True)\n",
        "os.makedirs(report_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e01d255",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e01d255",
        "outputId": "bf9e1eb6-19a2-49ce-ac93-e3b2b0b5a27b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running experiment with config: {'learning_rate': 1e-05, 'weight_decay': 0.01}\n"
          ]
        }
      ],
      "source": [
        "config = hyperparameter_grid[experiment_index]\n",
        "print(f\"Running experiment with config: {config}\")\n",
        "\n",
        "df = pd.read_csv(os.path.join(data_path, \"clean_data_no_stop.csv\"))\n",
        "df[\"cleaned text\"] = df[\"cleaned text\"].astype(str)\n",
        "emails = df['cleaned text'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    emails, labels, test_size=0.15, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "k = 5\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "all_fold_metrics = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a28acc5f",
      "metadata": {
        "id": "a28acc5f"
      },
      "source": [
        "## ðŸ”¹ Cross-Validation Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc15756e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fc15756e",
        "outputId": "26569c89-2021-47bc-cdb6-699225c8be8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Fold 1/5 =====\n",
            "Tokenizing 60261 emails with max_length=320...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved tokenized data to '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data/train_fold1.pt'\n",
            "Tokenizing 15066 emails with max_length=320...\n",
            "Saved tokenized data to '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data/val_fold1.pt'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Model on fold 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjessica-borowy-1\u001b[0m (\u001b[33mjessica-borowy-1-georgia-tech-yellow-jackets\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250620_205907-a4n19m5q</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jessica-borowy-1-georgia-tech-yellow-jackets/huggingface/runs/a4n19m5q' target=\"_blank\">neat-wood-16</a></strong> to <a href='https://wandb.ai/jessica-borowy-1-georgia-tech-yellow-jackets/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jessica-borowy-1-georgia-tech-yellow-jackets/huggingface' target=\"_blank\">https://wandb.ai/jessica-borowy-1-georgia-tech-yellow-jackets/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jessica-borowy-1-georgia-tech-yellow-jackets/huggingface/runs/a4n19m5q' target=\"_blank\">https://wandb.ai/jessica-borowy-1-georgia-tech-yellow-jackets/huggingface/runs/a4n19m5q</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11301' max='11301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11301/11301 24:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Roc Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.061600</td>\n",
              "      <td>0.034616</td>\n",
              "      <td>0.991239</td>\n",
              "      <td>0.992110</td>\n",
              "      <td>0.995800</td>\n",
              "      <td>0.988447</td>\n",
              "      <td>0.999455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.061800</td>\n",
              "      <td>0.026019</td>\n",
              "      <td>0.994624</td>\n",
              "      <td>0.995185</td>\n",
              "      <td>0.993472</td>\n",
              "      <td>0.996903</td>\n",
              "      <td>0.999602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.030040</td>\n",
              "      <td>0.994823</td>\n",
              "      <td>0.995346</td>\n",
              "      <td>0.997369</td>\n",
              "      <td>0.993330</td>\n",
              "      <td>0.999658</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold1_results/checkpoint-3767 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold1_results/checkpoint-7534 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold1_results/checkpoint-11301 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving the model\n",
            "Obtaining fold metrics for fold 1\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 78 misclassified emails to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold1_misclassified_emails.csv\n",
            "\n",
            "Classification report for Fold 1 Validation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Legit     0.9916    0.9967    0.9942      6670\n",
            "       Phish     0.9974    0.9933    0.9953      8396\n",
            "\n",
            "    accuracy                         0.9948     15066\n",
            "   macro avg     0.9945    0.9950    0.9948     15066\n",
            "weighted avg     0.9948    0.9948    0.9948     15066\n",
            "\n",
            "Saved classification report to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold1_val_classification_report.txt\n",
            "\n",
            "===== Fold 2/5 =====\n",
            "Tokenizing 60261 emails with max_length=320...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved tokenized data to '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data/train_fold2.pt'\n",
            "Tokenizing 15066 emails with max_length=320...\n",
            "Saved tokenized data to '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data/val_fold2.pt'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Model on fold 2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11301' max='11301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11301/11301 24:20, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Roc Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.078185</td>\n",
              "      <td>0.981946</td>\n",
              "      <td>0.983569</td>\n",
              "      <td>0.997916</td>\n",
              "      <td>0.969628</td>\n",
              "      <td>0.999407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.017100</td>\n",
              "      <td>0.025388</td>\n",
              "      <td>0.994358</td>\n",
              "      <td>0.994930</td>\n",
              "      <td>0.996535</td>\n",
              "      <td>0.993330</td>\n",
              "      <td>0.999729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.030226</td>\n",
              "      <td>0.994624</td>\n",
              "      <td>0.995170</td>\n",
              "      <td>0.996536</td>\n",
              "      <td>0.993807</td>\n",
              "      <td>0.999751</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold2_results/checkpoint-3767 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving the model\n",
            "Obtaining fold metrics for fold 2\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 81 misclassified emails to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold2_misclassified_emails.csv\n",
            "\n",
            "Classification report for Fold 2 Validation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Legit     0.9922    0.9957    0.9939      6670\n",
            "       Phish     0.9965    0.9938    0.9952      8396\n",
            "\n",
            "    accuracy                         0.9946     15066\n",
            "   macro avg     0.9944    0.9947    0.9946     15066\n",
            "weighted avg     0.9946    0.9946    0.9946     15066\n",
            "\n",
            "Saved classification report to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold2_val_classification_report.txt\n",
            "\n",
            "===== Fold 3/5 =====\n",
            "Tokenizing 60262 emails with max_length=320...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved tokenized data to '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data/train_fold3.pt'\n",
            "Tokenizing 15065 emails with max_length=320...\n",
            "Saved tokenized data to '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data/val_fold3.pt'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Model on fold 3\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11301' max='11301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11301/11301 24:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Roc Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.038242</td>\n",
              "      <td>0.991238</td>\n",
              "      <td>0.992147</td>\n",
              "      <td>0.991203</td>\n",
              "      <td>0.993092</td>\n",
              "      <td>0.999577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.034747</td>\n",
              "      <td>0.992035</td>\n",
              "      <td>0.992820</td>\n",
              "      <td>0.997595</td>\n",
              "      <td>0.988090</td>\n",
              "      <td>0.999753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.033805</td>\n",
              "      <td>0.994092</td>\n",
              "      <td>0.994690</td>\n",
              "      <td>0.996533</td>\n",
              "      <td>0.992854</td>\n",
              "      <td>0.999766</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving the model\n",
            "Obtaining fold metrics for fold 3\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 89 misclassified emails to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold3_misclassified_emails.csv\n",
            "\n",
            "Classification report for Fold 3 Validation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Legit     0.9910    0.9957    0.9933      6669\n",
            "       Phish     0.9965    0.9929    0.9947      8396\n",
            "\n",
            "    accuracy                         0.9941     15065\n",
            "   macro avg     0.9938    0.9943    0.9940     15065\n",
            "weighted avg     0.9941    0.9941    0.9941     15065\n",
            "\n",
            "Saved classification report to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold3_val_classification_report.txt\n",
            "\n",
            "===== Fold 4/5 =====\n",
            "Tokenizing 60262 emails with max_length=320...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved tokenized data to '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data/train_fold4.pt'\n",
            "Tokenizing 15065 emails with max_length=320...\n",
            "Saved tokenized data to '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data/val_fold4.pt'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Model on fold 4\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11301' max='11301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11301/11301 24:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Roc Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.041900</td>\n",
              "      <td>0.039195</td>\n",
              "      <td>0.990176</td>\n",
              "      <td>0.991142</td>\n",
              "      <td>0.996150</td>\n",
              "      <td>0.986184</td>\n",
              "      <td>0.999468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.047700</td>\n",
              "      <td>0.031922</td>\n",
              "      <td>0.993495</td>\n",
              "      <td>0.994146</td>\n",
              "      <td>0.997244</td>\n",
              "      <td>0.991067</td>\n",
              "      <td>0.999715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.026787</td>\n",
              "      <td>0.995154</td>\n",
              "      <td>0.995651</td>\n",
              "      <td>0.995948</td>\n",
              "      <td>0.995355</td>\n",
              "      <td>0.999759</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving the model\n",
            "Obtaining fold metrics for fold 4\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 73 misclassified emails to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold4_misclassified_emails.csv\n",
            "\n",
            "Classification report for Fold 4 Validation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Legit     0.9942    0.9949    0.9945      6669\n",
            "       Phish     0.9959    0.9954    0.9957      8396\n",
            "\n",
            "    accuracy                         0.9952     15065\n",
            "   macro avg     0.9951    0.9951    0.9951     15065\n",
            "weighted avg     0.9952    0.9952    0.9952     15065\n",
            "\n",
            "Saved classification report to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold4_val_classification_report.txt\n",
            "\n",
            "===== Fold 5/5 =====\n",
            "Tokenizing 60262 emails with max_length=320...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved tokenized data to '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data/train_fold5.pt'\n",
            "Tokenizing 15065 emails with max_length=320...\n",
            "Saved tokenized data to '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data/val_fold5.pt'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Model on fold 5\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11301' max='11301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11301/11301 24:30, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Roc Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.083900</td>\n",
              "      <td>0.072524</td>\n",
              "      <td>0.980352</td>\n",
              "      <td>0.982067</td>\n",
              "      <td>0.999260</td>\n",
              "      <td>0.965456</td>\n",
              "      <td>0.999270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.021916</td>\n",
              "      <td>0.996150</td>\n",
              "      <td>0.996549</td>\n",
              "      <td>0.995483</td>\n",
              "      <td>0.997618</td>\n",
              "      <td>0.999754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.021635</td>\n",
              "      <td>0.996216</td>\n",
              "      <td>0.996603</td>\n",
              "      <td>0.997257</td>\n",
              "      <td>0.995950</td>\n",
              "      <td>0.999788</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving the model\n",
            "Obtaining fold metrics for fold 5\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 57 misclassified emails to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold5_misclassified_emails.csv\n",
            "\n",
            "Classification report for Fold 5 Validation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Legit     0.9949    0.9966    0.9957      6670\n",
            "       Phish     0.9973    0.9959    0.9966      8395\n",
            "\n",
            "    accuracy                         0.9962     15065\n",
            "   macro avg     0.9961    0.9963    0.9962     15065\n",
            "weighted avg     0.9962    0.9962    0.9962     15065\n",
            "\n",
            "Saved classification report to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold5_val_classification_report.txt\n"
          ]
        }
      ],
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_val, y_train_val), 1):\n",
        "        print(f\"\\n===== Fold {fold}/{k} =====\")\n",
        "\n",
        "        X_train_fold = [X_train_val[i] for i in train_idx]\n",
        "        y_train_fold = [y_train_val[i] for i in train_idx]\n",
        "        X_val_fold = [X_train_val[i] for i in val_idx]\n",
        "        y_val_fold = [y_train_val[i] for i in val_idx]\n",
        "\n",
        "        # Tokenize fold data\n",
        "        train_tokens = tokenize_and_cache(\n",
        "            X_train_fold,\n",
        "            tokenizer_name='microsoft/deberta-v3-base',\n",
        "            save_path=os.path.join(Deberta_data_path, f\"train_fold{fold}.pt\"),\n",
        "            force_retokenize=True,\n",
        "        )\n",
        "        val_tokens = tokenize_and_cache(\n",
        "            X_val_fold,\n",
        "            tokenizer_name='microsoft/deberta-v3-base',\n",
        "            save_path=os.path.join(Deberta_data_path, f\"val_fold{fold}.pt\"),\n",
        "            force_retokenize=True,\n",
        "        )\n",
        "\n",
        "        # Create datasets with labels\n",
        "        train_dataset = Dataset.from_dict(train_tokens).add_column(\"labels\", y_train_fold)\n",
        "        val_dataset = Dataset.from_dict(val_tokens).add_column(\"labels\", y_val_fold)\n",
        "        train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "        val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "        set_seed(42)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=2)\n",
        "\n",
        "        fold_output_dir = os.path.join(report_dir, f\"fold{fold}_results\")\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=fold_output_dir,\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            learning_rate=config[\"learning_rate\"],\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=16,\n",
        "            num_train_epochs=3,\n",
        "            weight_decay=config[\"weight_decay\"],\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"f1\",\n",
        "            greater_is_better=True,\n",
        "            logging_dir=os.path.join(report_dir, \"logs\"),\n",
        "            logging_steps=10,\n",
        "            fp16=True\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        "        )\n",
        "\n",
        "        print(f\"Training Model on fold {fold}\")\n",
        "        trainer.train()\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "        tokenizer.save_pretrained(os.path.join(report_dir, f\"phishing-deberta-model_config_{experiment_index}_fold_{fold}\"))\n",
        "        print(\"Saving the model\")\n",
        "        # Save the model\n",
        "        trainer.save_model(os.path.join(report_dir, f\"phishing-deberta-model_config_{experiment_index}_fold_{fold}\"))\n",
        "\n",
        "        print(f\"Obtaining fold metrics for fold {fold}\")\n",
        "        fold_metrics_over_epochs = []\n",
        "        for entry in trainer.state.log_history:\n",
        "            if \"eval_f1\" in entry and \"epoch\" in entry:\n",
        "                fold_metrics_over_epochs.append({\n",
        "                    \"fold\": fold,\n",
        "                    \"epoch\": entry[\"epoch\"],\n",
        "                    \"f1\": entry[\"eval_f1\"],\n",
        "                    \"accuracy\": entry.get(\"eval_accuracy\"),\n",
        "                    \"precision\": entry.get(\"eval_precision\"),\n",
        "                    \"recall\": entry.get(\"eval_recall\"),\n",
        "                })\n",
        "        all_fold_metrics.extend(fold_metrics_over_epochs)\n",
        "\n",
        "        # Extract optimizer info\n",
        "        optimizer_type = type(trainer.optimizer).__name__\n",
        "        optimizer_params = trainer.optimizer.param_groups[0]  # Get the first param group\n",
        "\n",
        "        learning_rate = optimizer_params.get(\"lr\", \"N/A\")\n",
        "        weight_decay = training_args.weight_decay\n",
        "        betas = optimizer_params.get(\"betas\", (\"N/A\", \"N/A\"))\n",
        "        epsilon = training_args.adam_epsilon if hasattr(training_args, \"adam_epsilon\") else \"N/A\"\n",
        "\n",
        "        # DeBERTa uses CrossEntropyLoss for classification by default\n",
        "        loss_type = \"CrossEntropyLoss\"\n",
        "\n",
        "        # Classification report & confusion matrix for validation set\n",
        "        val_preds = trainer.predict(val_dataset)\n",
        "        y_val_pred = val_preds.predictions.argmax(axis=1)\n",
        "        y_val_true = val_preds.label_ids\n",
        "\n",
        "        #Find misclassified indices\n",
        "        misclassified_indices = [i for i, (pred, true) in enumerate(zip(y_val_pred, y_val_true)) if pred != true]\n",
        "\n",
        "        logits = val_preds.predictions\n",
        "        probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "        confidences = probs.max(axis=1)\n",
        "\n",
        "        # Save misclassified emails\n",
        "        misclassified_data = {\n",
        "            \"Email\": [X_val_fold[i] for i in misclassified_indices],\n",
        "            \"True Label\": [y_val_fold[i] for i in misclassified_indices],\n",
        "            \"Predicted Label\": [y_val_pred[i] for i in misclassified_indices],\n",
        "            \"Confidence\": [confidences[i] for i in misclassified_indices]\n",
        "        }\n",
        "\n",
        "        df_errors = pd.DataFrame(misclassified_data)\n",
        "        error_csv_path = os.path.join(report_dir, f\"fold{fold}_misclassified_emails.csv\")\n",
        "        df_errors.to_csv(error_csv_path, index=False)\n",
        "        print(f\"Saved {len(df_errors)} misclassified emails to {error_csv_path}\")\n",
        "\n",
        "        val_class_report = classification_report(\n",
        "            y_val_true, y_val_pred, target_names=[\"Legit\", \"Phish\"], digits=4\n",
        "        )\n",
        "        # Full report\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        report_text = f\"\"\"\n",
        "        ==================== EVALUATION REPORT ====================\n",
        "\n",
        "        Timestamp       : {timestamp}\n",
        "        Model           : microsoft/deberta-v3-base\n",
        "        Tokenizer       : microsoft/deberta-v3-base\n",
        "        Token Length    : 320\n",
        "        Max Epochs      : {training_args.num_train_epochs}\n",
        "        Best Metric     : {training_args.metric_for_best_model}\n",
        "\n",
        "        ---------------- Optimizer & Loss Info ----------------\n",
        "        Optimizer       : {optimizer_type}\n",
        "        Learning Rate   : {learning_rate}\n",
        "        Weight Decay    : {weight_decay}\n",
        "        Betas           : {betas}\n",
        "        Epsilon         : {epsilon}\n",
        "        Loss Function   : {loss_type}\n",
        "        ---------------- Classification Report ----------------\n",
        "        {val_class_report}\n",
        "        \"\"\"\n",
        "        print(f\"\\nClassification report for Fold {fold} Validation:\\n{val_class_report}\")\n",
        "\n",
        "        report_path = os.path.join(report_dir, f\"fold{fold}_val_classification_report.txt\")\n",
        "        save_classification_report(report_text, report_path)\n",
        "\n",
        "        cm_val = confusion_matrix(y_val_true, y_val_pred)\n",
        "        disp_val = ConfusionMatrixDisplay(cm_val, display_labels=[\"Legit\", \"Phish\"])\n",
        "        disp_val.plot(cmap='Blues')\n",
        "        plt.title(f\"Confusion Matrix - Validation Fold {fold}\")\n",
        "        plt.savefig(os.path.join(report_dir, f\"fold{fold}_val_confusion_matrix.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Explicitly delete model, datasets, and predictions\n",
        "        del model\n",
        "        del train_dataset\n",
        "        del val_dataset\n",
        "        del train_tokens\n",
        "        del val_tokens\n",
        "        del val_preds\n",
        "\n",
        "        # remove globally cached tokenizer\n",
        "        if 'tokenizer' in globals():\n",
        "          del tokenizer\n",
        "\n",
        "        # Force garbage collection and clear CUDA cache\n",
        "        import gc\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51CY_48hUAlE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51CY_48hUAlE",
        "outputId": "405df999-0a8b-4b0b-9499-36f0db5f2cda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold metrics over time saved to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold_metrics_over_time.csv\n"
          ]
        }
      ],
      "source": [
        "# Aggregate and save fold metrics over time\n",
        "metrics_df = pd.DataFrame(all_fold_metrics)\n",
        "fold_metrics_path = os.path.join(report_dir, \"fold_metrics_over_time.csv\")\n",
        "metrics_df.to_csv(fold_metrics_path, index=False)\n",
        "print(f\"Fold metrics over time saved to {fold_metrics_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sDrn-pNGUJhC",
      "metadata": {
        "id": "sDrn-pNGUJhC"
      },
      "outputs": [],
      "source": [
        "# Plot F1 and other metrics over epochs per fold\n",
        "# Use a high-contrast palette\n",
        "palette = sns.color_palette(\"colorblind\", n_colors=metrics_df['fold'].nunique())\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=metrics_df, x=\"epoch\", y=\"f1\", hue=\"fold\", palette=palette, marker=\"o\")\n",
        "plt.title(\"F1 Score over Epochs per Fold\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.grid(True)\n",
        "plt.legend(title=\"Fold\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(report_dir, \"f1_over_time_across_folds.png\"))\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g8NR1YrdUTw3",
      "metadata": {
        "id": "g8NR1YrdUTw3"
      },
      "outputs": [],
      "source": [
        "for metric in [\"accuracy\", \"precision\", \"recall\"]:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.lineplot(data=metrics_df, x=\"epoch\", y=metric, hue=\"fold\", palette=palette, marker=\"o\")\n",
        "        plt.title(f\"{metric.capitalize()} over Epochs per Fold\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(metric.capitalize())\n",
        "        plt.grid(True)\n",
        "        plt.legend(title=\"Fold\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(report_dir, f\"{metric}_over_time_across_folds.png\"))\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lXfkc_AsUXO3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXfkc_AsUXO3",
        "outputId": "d86f9739-9125-4d7e-d34f-c474db1e10c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best model: Fold 5 at epoch 3 with F1=0.9966\n"
          ]
        }
      ],
      "source": [
        "best_row = metrics_df.loc[metrics_df['f1'].idxmax()]\n",
        "best_fold = int(best_row['fold'])\n",
        "best_epoch = int(best_row['epoch'])\n",
        "\n",
        "print(f\"\\nBest model: Fold {best_fold} at epoch {best_epoch} with F1={best_row['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1W_MioJUcRz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1W_MioJUcRz",
        "outputId": "ffab6a14-a273-401e-8bfa-e2818aa6d636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading best checkpoint from: /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/fold5_results/checkpoint-11301\n"
          ]
        }
      ],
      "source": [
        "best_checkpoint_path = trainer.state.best_model_checkpoint\n",
        "print(f\"Loading best checkpoint from: {best_checkpoint_path}\")\n",
        "best_model = AutoModelForSequenceClassification.from_pretrained(best_checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47853810",
      "metadata": {
        "id": "47853810"
      },
      "source": [
        "## ðŸ”¹ Final Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da8a1b32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da8a1b32",
        "outputId": "d6ef71af-a0b4-4e29-c1c7-e5554eb76d34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing 13294 emails with max_length=320...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved tokenized data to '/content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Deberta_data/test_set.pt'\n"
          ]
        }
      ],
      "source": [
        "# Prepare test dataset\n",
        "test_tokens = tokenize_and_cache(\n",
        "    X_test,\n",
        "    tokenizer_name=\"microsoft/deberta-v3-base\",\n",
        "    save_path=os.path.join(Deberta_data_path, \"test_set.pt\"),\n",
        "    force_retokenize=True,\n",
        ")\n",
        "test_dataset = Dataset.from_dict(test_tokens).add_column(\"labels\", y_test)\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ltJUZ0V3U2MX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltJUZ0V3U2MX",
        "outputId": "a1a8a237-c276-4496-e6c6-5c3bf248c80e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Evaluate best model on test set\n",
        "test_training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(report_dir, \"test_results\"),\n",
        "    per_device_eval_batch_size=64,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    fp16=True,\n",
        ")\n",
        "test_trainer = Trainer(\n",
        "    model=best_model,\n",
        "    args=test_training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wObq2cNAVLcN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "wObq2cNAVLcN",
        "outputId": "e9cde0a8-22b0-49a1-a7fb-4bac1aced811"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='208' max='208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [208/208 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set evaluation metrics:\n",
            "{'eval_loss': 0.022691208869218826, 'eval_accuracy': 0.9954866857228825, 'eval_f1': 0.9959470413401783, 'eval_precision': 0.9968897903989182, 'eval_recall': 0.9950060736941557, 'eval_roc_auc': 0.9998758542189554, 'eval_runtime': 24.0829, 'eval_samples_per_second': 552.011, 'eval_steps_per_second': 8.637}\n"
          ]
        }
      ],
      "source": [
        "test_metrics = test_trainer.evaluate(test_dataset)\n",
        "print(\"\\nTest set evaluation metrics:\")\n",
        "print(test_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JDTOqw02VXjD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDTOqw02VXjD",
        "outputId": "0cb43640-cce6-469a-a5f9-b307637a4286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving the model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "print(\"Saving the model\")\n",
        "# Save the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "tokenizer.save_pretrained(os.path.join(report_dir, f\"best_phishing-deberta-model_config_{experiment_index}\"))\n",
        "test_trainer.save_model(os.path.join(report_dir, f\"best_phishing-deberta-model_config_{experiment_index}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o8PKWQoNVQnz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "o8PKWQoNVQnz",
        "outputId": "468f209f-0d88-4682-af89-392a3c6394fb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_preds = test_trainer.predict(test_dataset)\n",
        "y_test_pred = test_preds.predictions.argmax(axis=1)\n",
        "y_test_true = test_preds.label_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jU5Cx-txVT0N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU5Cx-txVT0N",
        "outputId": "e6d7885e-2797-48d7-bbb2-b53e987ee112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC Curve graph saved at /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/roc_curve_test_set.png\n"
          ]
        }
      ],
      "source": [
        "# ROC Curve Visualization\n",
        "probs = test_preds.predictions[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test_true, probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "roc_path = os.path.join(report_dir, \"roc_curve_test_set.png\")\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Test Set\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig(roc_path)\n",
        "plt.close()\n",
        "\n",
        "print(f\"ROC Curve graph saved at {roc_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Hxxw52YVlFe",
      "metadata": {
        "id": "6Hxxw52YVlFe"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test_true, y_test_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Legit\", \"Phish\"])\n",
        "disp.plot(cmap='Blues', xticks_rotation=45)\n",
        "plt.title(\"Confusion Matrix - Phishing Detection\")\n",
        "plt.tight_layout()\n",
        "cm_path1 = os.path.join(report_dir, \"test_confusion_matrix.png\")\n",
        "plt.savefig(cm_path1)\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I5qaMmGhVpCX",
      "metadata": {
        "id": "I5qaMmGhVpCX"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix (normalized)\n",
        "cm = confusion_matrix(y_test_true, y_test_pred, normalize='true')  # normalize by true labels (rows)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Legit\", \"Phish\"])\n",
        "disp.plot(cmap='Blues', xticks_rotation=45)\n",
        "plt.title(\"Normalized Confusion Matrix - Phishing Detection\")\n",
        "plt.tight_layout()\n",
        "cm_path2 = os.path.join(report_dir, \"test_confusion_matrix_normalized.png\")\n",
        "plt.savefig(cm_path2)\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nK02alhPVu45",
      "metadata": {
        "id": "nK02alhPVu45"
      },
      "outputs": [],
      "source": [
        "test_class_report = classification_report(\n",
        "    y_test_true, y_test_pred, target_names=[\"Legit\", \"Phish\"], digits=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QBpVhXJtVxwZ",
      "metadata": {
        "id": "QBpVhXJtVxwZ"
      },
      "outputs": [],
      "source": [
        "# Full report\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "report_text = f\"\"\"\n",
        "==================== EVALUATION REPORT ====================\n",
        "\n",
        "Timestamp       : {timestamp}\n",
        "Model           : microsoft/deberta-v3-base\n",
        "Tokenizer       : microsoft/deberta-v3-base\n",
        "Token Length    : 320\n",
        "Max Epochs      : {training_args.num_train_epochs}\n",
        "Best Metric     : {training_args.metric_for_best_model}\n",
        "\n",
        "---------------- Optimizer & Loss Info ----------------\n",
        "Optimizer       : {optimizer_type}\n",
        "Learning Rate   : {learning_rate}\n",
        "Weight Decay    : {weight_decay}\n",
        "Betas           : {betas}\n",
        "Epsilon         : {epsilon}\n",
        "Loss Function   : {loss_type}\n",
        "\n",
        "---------------- Evaluation Metrics ----------------\n",
        "{json.dumps(test_metrics, indent=4)}\n",
        "\n",
        "ROC Curve visualization saved at: {roc_path}\n",
        "\n",
        "---------------- Classification Report ----------------\n",
        "{test_class_report}\n",
        "\n",
        "Confusion Matrix Saved at: {cm_path1}\n",
        "Normalized Confusion Matrix Saved at: {cm_path2}\n",
        "\n",
        "============================================================\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r-SESw76V323",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-SESw76V323",
        "outputId": "9aa3836a-32bd-40a2-a227-676591c97bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification report for Test Set:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       Legit     0.9937    0.9961    0.9949      5885\n",
            "       Phish     0.9969    0.9950    0.9959      7409\n",
            "\n",
            "    accuracy                         0.9955     13294\n",
            "   macro avg     0.9953    0.9955    0.9954     13294\n",
            "weighted avg     0.9955    0.9955    0.9955     13294\n",
            "\n",
            "Saved classification report to /content/drive/My Drive/Cybersecurity Practicum/CV_phishing_results/Newer_Deberta_results/config_4/test_set_classification_report.txt\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nClassification report for Test Set:\\n\", test_class_report)\n",
        "test_report_path = os.path.join(report_dir, \"test_set_classification_report.txt\")\n",
        "save_classification_report(report_text, test_report_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
