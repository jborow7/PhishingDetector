# -*- coding: utf-8 -*-
"""Deberta_Full_Optimization_Colab_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ysWDjbcd6SF7TqOIF5sYWAE2vVjqjCeA
"""

pip install transformers==4.37.2 peft==0.10.0

# Install required packages
!pip install -q datasets torch scikit-learn matplotlib

pip install optimum[onnxruntime]

!pip install xgboost

pip install accelerate==0.27.2

pip uninstall onnxruntime

!pip install onnxruntime-gpu

pip show onnxruntime-gpu

import onnxruntime as ort
print(ort.get_available_providers())

# Load fine-tuned model and tokenizer
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel, set_seed
import torch
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import time
import onnxruntime as ort
from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType
import os
import torch.quantization
from torch.utils.data import Dataset
import torch.nn.functional as F
import torch.nn as nn
import joblib
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import xgboost as xgb

from sklearn.metrics import accuracy_score, f1_score, classification_report
import time
from torch.utils.data import DataLoader, TensorDataset

def evaluate_model(model, tokenizer, samples, labels, extract_features, name="Model", output_dir="./", batch_size=4, device="cuda"):
    device = torch.device(device)
    model.to(device)
    model.eval()

    # Preprocess input
    features = torch.tensor([extract_features(t) for t in samples], dtype=torch.float32)
    tokenized = tokenizer(samples, return_tensors="pt", padding=True, truncation=True, max_length=512)

    dataset = TensorDataset(
        tokenized["input_ids"],
        tokenized["attention_mask"],
        features,
        torch.tensor(labels)
    )

    loader = DataLoader(dataset, batch_size=batch_size)

    all_preds = []
    all_labels = []

    start = time.time()
    with torch.no_grad():
        for batch in loader:
            input_ids, attention_mask, batch_features, batch_labels = [x.to(device) for x in batch]
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, features=batch_features)
            logits = outputs["logits"]
            preds = torch.argmax(F.softmax(logits, dim=1), dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(batch_labels.cpu().numpy())
    end = time.time()

    # Metrics
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    report = classification_report(all_labels, all_preds, output_dict=False)
    cm = confusion_matrix(all_labels, all_preds)

    # Save classification report
    os.makedirs(output_dir, exist_ok=True)
    report_path = os.path.join(output_dir, f"{name.lower()}_classification_report.txt")
    with open(report_path, "w") as f:
        f.write(report)

    # Plot confusion matrix
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Legit", "Phishing"], yticklabels=["Legit", "Phishing"])
    plt.title(f"{name} Confusion Matrix")
    plt.ylabel("True Label")
    plt.xlabel("Predicted Label")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{name.lower()}_confusion_matrix.png"))
    plt.close()

    print(f"\n{name} Evaluation:")
    print(f"Accuracy: {acc:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"Inference Time: {end - start:.4f} sec")
    print(report)

    return {
        "name": name,
        "accuracy": acc,
        "f1": f1,
        "inference_time": end - start
    }

def evaluate_text_only_model(model, tokenizer, samples, labels, batch_size=32, device="cuda", name="Model", output_dir="./"):
    device = torch.device(device)
    model.to(device)
    model.eval()

    tokenized = tokenizer(samples, return_tensors="pt", padding=True, truncation=True, max_length=512)

    dataset = TensorDataset(
        tokenized["input_ids"],
        tokenized["attention_mask"],
        torch.tensor(labels)
    )

    loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=2)

    all_preds = []
    all_labels = []

    import time
    start = time.time()
    with torch.inference_mode():
        for batch in loader:
            input_ids, attention_mask, batch_labels = [x.to(device) for x in batch]
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(batch_labels.cpu().numpy())
    end = time.time()

    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    report = classification_report(all_labels, all_preds)
    cm = confusion_matrix(all_labels, all_preds)

    os.makedirs(output_dir, exist_ok=True)
    with open(os.path.join(output_dir, f"{name.lower()}_classification_report.txt"), "w") as f:
        f.write(report)

    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Legit","Phishing"], yticklabels=["Legit","Phishing"])
    plt.title(f"{name} Confusion Matrix")
    plt.ylabel("True Label")
    plt.xlabel("Predicted Label")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{name.lower()}_confusion_matrix.png"))
    plt.close()

    print(f"\n{name} Evaluation:")
    print(f"Accuracy: {acc:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"Inference Time: {end - start:.2f} sec")
    print(report)

    return {
        "name": name,
        "accuracy": acc,
        "f1": f1,
        "inference_time": end - start
    }

def evaluate_classical_model(model, X, y_true, name="Model", output_dir=None):
    start = time.time()
    y_pred = model.predict(X)
    end = time.time()

    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    elapsed = end - start

    print(f"output_dir = {output_dir}")
    print(f"\n{name} Results:")
    print(f"  Accuracy:     {acc:.4f}")
    print(f"  F1 Score:     {f1:.4f}")
    print(f"  Inference Time (batch): {elapsed:.4f} sec")

    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap="Blues", values_format='d')
    plt.title(f"{name} Confusion Matrix")
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        plt.savefig(f"{output_dir}/{name.lower().replace(' ', '_')}_confusion_matrix.png")
    plt.show()

    report = classification_report(y_true, y_pred)
    print(report)

    if output_dir:
        with open(f"{output_dir}/{name.lower().replace(' ', '_')}_report.txt", "w") as f:
            f.write(report)

    return {"name": name, "accuracy": acc, "f1": f1, "inference_time": elapsed}

# Dataset class
class EmailDataset(Dataset):
    def __init__(self, df):
        self.encodings = tokenizer(list(df["cleaned text"]), truncation=True, padding="max_length", max_length=512)
        self.features = df[["link_count", "email length"]].values
        assert np.isfinite(self.features).all(), "Detected NaN or Inf in input features"
        self.labels = df["label"].values.astype(int)
        assert not np.isnan(self.labels).any(), "NaNs in labels"

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["features"] = torch.tensor(self.features[idx], dtype=torch.float32)
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

# Model class
class DebertaWithFeatures(nn.Module):
    def __init__(self):
        super().__init__()
        self.deberta = AutoModel.from_pretrained("microsoft/deberta-v3-base")
        self.config = self.deberta.config
        self.config.return_dict = True
        self.hidden_size = self.deberta.config.hidden_size
        self.feature_proj = nn.Linear(2, 32)
        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_size + 32, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 2)
        )

    def forward(self, input_ids, attention_mask, features=None, labels=None):
      deberta_output = self.deberta(input_ids=input_ids, attention_mask=attention_mask)

      if self.config.return_dict:
          cls_output = deberta_output.last_hidden_state[:, 0]
      else:
          cls_output = deberta_output[0][:, 0]  # CLS token

      # Handle optional or empty features
      if features is not None and features.shape[1] > 0:
          feature_proj = self.feature_proj(features)
          feature_proj = torch.clamp(feature_proj, -10.0, 10.0)
          combined = torch.cat((cls_output, feature_proj), dim=1)
      else:
          combined = cls_output  # Only use CLS output when no features are passed

      logits = self.classifier(combined)

      # Debug for NaNs or infs
      if torch.isnan(logits).any() or torch.isinf(logits).any():
          print("NaNs in logits at batch")
          print("CLS shape:", cls_output.shape)
          print("Features:", features[0] if features is not None and features.shape[0] > 0 else "None")

      # Optional loss
      loss = None
      if labels is not None:
          loss_fn = nn.CrossEntropyLoss()
          loss = loss_fn(logits, labels)

      return {"loss": loss, "logits": logits, "labels": labels}

def collate_fn(batch):
    return {
        "input_ids": torch.stack([torch.tensor(x["input_ids"]) for x in batch]),
        "attention_mask": torch.stack([torch.tensor(x["attention_mask"]) for x in batch])
    }

# Extract structured features
def extract_features(email):
    return [email.count("http"), len(email)]

def save_classification_report(report_str, filename):
    with open(filename, 'w') as f:
        f.write(report_str)
    print(f"Saved classification report to {filename}")

class DebertaONNXWrapper(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, attention_mask, features):
        out = self.model(input_ids=input_ids, attention_mask=attention_mask, features=features)
        return out["logits"]  # Return only tensor for ONNX export

def evaluate_onnx_model(onnx_model_path, tokenizer, samples, labels, extract_features, name="ONNX", output_dir="./", batch_size=32):
    import time
    import os
    import numpy as np
    import onnxruntime as ort
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
    from tqdm import tqdm

    # Precompute features for all samples
    all_features = np.array([extract_features(s) for s in samples], dtype=np.float32)

    # Initialize ONNX session with optimization
    sess_options = ort.SessionOptions()
    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = ort.InferenceSession(onnx_model_path, sess_options)

    all_preds = []
    all_labels = []
    total_time = 0

    os.makedirs(output_dir, exist_ok=True)

    for i in tqdm(range(0, len(samples), batch_size), desc=f"Evaluating {name}"):
        batch_samples = samples[i:i+batch_size]
        batch_labels = labels[i:i+batch_size]
        batch_features = all_features[i:i+batch_size]

        tokenized = tokenizer(batch_samples, return_tensors="np", padding="max_length", truncation=True, max_length=512)

        onnx_inputs = {
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"],
            "features": batch_features,
        }

        start = time.time()
        onnx_outputs = session.run(None, onnx_inputs)
        end = time.time()
        total_time += (end - start)

        logits = onnx_outputs[0]
        preds = np.argmax(logits, axis=-1)

        all_preds.extend(preds.tolist())
        all_labels.extend(batch_labels)

    # Metrics & report plotting
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    report = classification_report(all_labels, all_preds)
    cm = confusion_matrix(all_labels, all_preds)

    report_path = os.path.join(output_dir, f"{name.lower()}_classification_report.txt")
    with open(report_path, "w") as f:
        f.write(report)

    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Purples", xticklabels=["Legit", "Phishing"], yticklabels=["Legit", "Phishing"])
    plt.title(f"{name} Confusion Matrix")
    plt.ylabel("True Label")
    plt.xlabel("Predicted Label")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{name.lower()}_confusion_matrix.png"))
    plt.close()

    print(f"\n{name} Evaluation:")
    print(f"Accuracy: {acc:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"Total Inference Time: {total_time:.4f} sec")
    print(report)

    return {
        "name": name,
        "accuracy": acc,
        "f1": f1,
        "inference_time": total_time
    }

def dummy_extract_features(text):
    return [0.0, 0.0]  # Matches expected input to self.feature_proj

# Mount Google Drive
from google.colab import drive
import os
drive.mount('/content/drive')
output_dir = "/content/drive/My Drive/Cybersecurity Practicum/Custom_CV_phishing_results/Performance_Comparison"
os.makedirs(output_dir, exist_ok=True)

model_path = "/content/drive/My Drive/Cybersecurity Practicum/Custom_CV_phishing_results/New_Deberta_results/config_4/best_model_fold_5"

model = DebertaWithFeatures()
# Load the state dict
state_dict = torch.load(os.path.join(model_path, "model.pt"))

model.load_state_dict(state_dict)
model.eval()
tokenizer = AutoTokenizer.from_pretrained(model_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

import pandas as pd
data_path = '/content/drive/My Drive/Cybersecurity Practicum/'
df = pd.read_csv(os.path.join(data_path, "finalized_dataset.csv"))
df["cleaned text"] = df["cleaned text"].astype(str)
# Ensure label column is integer type
df["label"] = df["label"].astype(int)

#Fix extreme outliers
df["link_count"] = df["link_count"].clip(0, 100)
df["email length"] = df["email length"].clip(0, 20000)

X_train, X_test, y_train, y_test = train_test_split(
    df[["cleaned text", "link_count", "email length"]],
    df["label"],
    test_size=0.2,
    stratify=df["label"],
    random_state=42
)

test_text = list(X_test["cleaned text"])
test_labels = list(y_test)

train_text = list(X_train["cleaned text"])
train_labels = list(y_train)

features_tensor = torch.tensor([extract_features(e) for e in train_text], dtype=torch.float32).to(device)

import torch
torch.cuda.empty_cache()

# Tokenize
tokens = tokenizer(train_text, return_tensors="pt", padding="max_length", truncation=True, max_length=512)

# Unpack inputs
input_ids = tokens["input_ids"]
attention_mask = tokens["attention_mask"]

import seaborn as sns
results = []

results.append(evaluate_model(
    model=model,
    tokenizer=tokenizer,
    samples=test_text,
    labels=test_labels,
    extract_features=dummy_extract_features,
    name="Original",
    output_dir=output_dir,
    batch_size=64,
    device="cuda"
))

export_model = DebertaONNXWrapper(model)
export_model.eval()
export_model.to(device)

# Dummy input
dummy_text = "This is a sample input for ONNX export."
dummy_features = torch.tensor([[0.0, len(dummy_text)]], dtype=torch.float32)  # shape: [1, num_features]
inputs = tokenizer(dummy_text, return_tensors="pt", padding="max_length", truncation=True, max_length=128)
input_ids = inputs["input_ids"]
attention_mask = inputs["attention_mask"]

input_ids = input_ids.to(device)
attention_mask = attention_mask.to(device)
dummy_features = dummy_features.to(device)

from tqdm import tqdm
# ONNX Export
from transformers.onnx import export
from transformers.onnx.features import FeaturesManager
import onnxruntime as ort
import numpy as np

onnx_model_dir = "/content/drive/My Drive/Cybersecurity Practicum/Custom_CV_phishing_results/GPU_ONNX_model"
os.makedirs(onnx_model_dir, exist_ok=True)
onnx_path = os.path.join(onnx_model_dir, "deberta_with_features.onnx")


torch.onnx.export(
    export_model,
    (input_ids, attention_mask, dummy_features),
    onnx_path,
    input_names=["input_ids", "attention_mask", "features"],
    output_names=["logits"],
    dynamic_axes={
        "input_ids": {0: "batch_size", 1: "seq_len"},
        "attention_mask": {0: "batch_size", 1: "seq_len"},
        "features": {0: "batch_size"},
        "logits": {0: "batch_size"},
    },
    do_constant_folding=True,
    opset_version=13,
)

print(f"ONNX model exported to {onnx_path}")

from onnxruntime.transformers.optimizer import optimize_model

# Optimize the model
optimized_model = optimize_model(onnx_path, model_type='bert')
# 'bert' optimization works well for transformer models like DeBERTa

optimized_model.save_model_to_file(os.path.join(onnx_model_dir, "deberta_with_features_optimized.onnx"))

class DebertaCalibrationDataReader(CalibrationDataReader):
    def __init__(self, texts, features, tokenizer, max_length=128):
        self.data = []
        for text, feat in zip(texts, features):
            assert len(feat) == 2  # Expected shape (2,)
            enc = tokenizer(
                text,
                return_tensors="np",
                padding="max_length",
                truncation=True,
                max_length=max_length,
            )
            self.data.append({
                "input_ids": enc["input_ids"].astype(np.int64),  # shape (1, seq_len)
                "attention_mask": enc["attention_mask"].astype(np.int64),  # shape (1, seq_len)
                "features": np.array([feat], dtype=np.float32),  # shape (1, 2)
            })
        self.iterator = iter(self.data)

    def get_next(self):
        return next(self.iterator, None)

# Calibration samples and features (from train set)
calib_texts = list(X_train["cleaned text"][:200])
calib_features = [extract_features(t) for t in calib_texts]

reader = DebertaCalibrationDataReader(calib_texts, calib_features, tokenizer)

sample = reader.get_next()
print("Sample batch for calibration:")
for k, v in sample.items():
    print(f"{k}: shape={v.shape}, dtype={v.dtype}")

calib_reader = DebertaCalibrationDataReader(calib_texts, calib_features, tokenizer)

from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantFormat, QuantType

calib_reader.iterator = iter(calib_reader.data)  # reset before passing to quantize_static

quantized_model_path = os.path.join(onnx_model_dir, "deberta_with_features_statically_quantized.onnx")
optimized_model_path = os.path.join(onnx_model_dir, "deberta_with_features_optimized.onnx")

quantize_static(
    model_input=optimized_model_path,
    model_output=quantized_model_path,
    calibration_data_reader=reader,
    quant_format=QuantFormat.QDQ,
    activation_type=QuantType.QUInt8,
    weight_type=QuantType.QInt8
)

print(f"✅ Static quantized model saved to {quantized_model_path}")

from onnxruntime.quantization import quantize_dynamic, QuantType

quantized_model_path = os.path.join(onnx_model_dir, "deberta_with_features_dynamically_quantized.onnx")
optimized_model_path = os.path.join(onnx_model_dir, "deberta_with_features_optimized.onnx")

quantize_dynamic(
    model_input=optimized_model_path,
    model_output=quantized_model_path,
    weight_type=QuantType.QInt8,
)

from onnxruntime.quantization import quantize_dynamic, QuantType

unoptimized_quantized_model_path = os.path.join(onnx_model_dir, "unoptimized_deberta_with_features_dynamically_quantized.onnx")
'''
quantize_dynamic(
    model_input=onnx_path,
    model_output=unoptimized_quantized_model_path,
    weight_type=QuantType.QInt8,
    extra_options={"DefaultTensorType": "QInt8"},  # Or QuantType.QUInt8 depending on your needs
)
'''

tokenizer = AutoTokenizer.from_pretrained(model_path)

def evaluate_onnx_model_fast(onnx_model_path, tokenizer, samples, labels, extract_features, name="ONNX", output_dir="./", batch_size=32):
    import time
    import os
    import numpy as np
    import onnxruntime as ort
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
    from tqdm import tqdm

    # Use optimized ONNX inference session
    session = ort.InferenceSession(
        onnx_model_path,
        providers=["CUDAExecutionProvider", "CPUExecutionProvider"]
    )

    # Precompute everything
    tokenized_all = tokenizer(samples, return_tensors="np", padding="max_length", truncation=True, max_length=512)
    all_features = np.array([extract_features(s) for s in samples], dtype=np.float32)

    # Setup ONNX session with optimized threading
    sess_options = ort.SessionOptions()
    sess_options.intra_op_num_threads = 2  # safe for Colab
    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = ort.InferenceSession(onnx_model_path, sess_options, providers=["CUDAExecutionProvider", "CPUExecutionProvider"])

    all_preds = []
    all_labels = []
    total_time = 0

    os.makedirs(output_dir, exist_ok=True)

    for i in tqdm(range(0, len(samples), batch_size), desc=f"Evaluating {name}"):
        input_ids = tokenized_all["input_ids"][i:i+batch_size]
        batch_labels = labels[i:i+batch_size]
        attention_mask = tokenized_all["attention_mask"][i:i+batch_size]
        features = all_features[i:i+batch_size]

        onnx_inputs = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "features": features,
        }

        # Inference timing
        start = time.time()
        logits = session.run(None, onnx_inputs)[0]
        end = time.time()
        total_time += (end - start)

        preds = np.atleast_1d(np.argmax(logits, axis=-1))

        all_preds.extend(preds.tolist())
        all_labels.extend(batch_labels)

    # Metrics & report plotting
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    report = classification_report(all_labels, all_preds)
    cm = confusion_matrix(all_labels, all_preds)

    report_path = os.path.join(output_dir, f"{name.lower()}_classification_report.txt")
    with open(report_path, "w") as f:
        f.write(report)

    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Purples", xticklabels=["Legit", "Phishing"], yticklabels=["Legit", "Phishing"])
    plt.title(f"{name} Confusion Matrix")
    plt.ylabel("True Label")
    plt.xlabel("Predicted Label")
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f"{name.lower()}_confusion_matrix.png"))
    plt.close()

    print(f"\n {name} Evaluation:")
    print(f"Accuracy: {acc:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"Total Inference Time: {total_time:.4f} sec")
    print(report)

    return {
        "name": name,
        "accuracy": acc,
        "f1": f1,
        "inference_time": total_time
    }

results.append(
    evaluate_onnx_model_fast(
        onnx_model_path=optimized_model_path,
        tokenizer=tokenizer,
        samples=test_text,
        labels=test_labels,
        extract_features=dummy_extract_features,
        name="Optimized ONNX",
        output_dir=onnx_model_dir,
        batch_size=64
    )
)

results.append(
    evaluate_onnx_model_fast(
        onnx_model_path=unoptimized_quantized_model_path,
        tokenizer=tokenizer,
        samples=test_text,
        labels=test_labels,
        extract_features=dummy_extract_features,
        name="Unoptimized Quantized ONNX",
        output_dir=onnx_model_dir,
        batch_size=64
    )
)

import onnx
from onnx import TensorProto

fixed_model_path = os.path.join(onnx_model_dir, "fixed_optimized_deberta_with_features_quantized.onnx")

model_path = quantized_model_path
model = onnx.load(model_path)

for node in model.graph.node:
    if node.op_type == "Cast":
        for attr in node.attribute:
            if attr.name == "to" and attr.type == onnx.AttributeProto.STRING:
                dtype_str = attr.s.decode("utf-8")  # e.g. "QInt8"
                # Map 'QInt8' to 'INT8'
                if dtype_str == "QInt8":
                    dtype_str = "INT8"
                mapping = {
                    "FLOAT": TensorProto.FLOAT,
                    "UINT8": TensorProto.UINT8,
                    "INT8": TensorProto.INT8,
                    "UINT16": TensorProto.UINT16,
                    "INT16": TensorProto.INT16,
                    "INT32": TensorProto.INT32,
                    "INT64": TensorProto.INT64,
                    "STRING": TensorProto.STRING,
                    "BOOL": TensorProto.BOOL,
                    # add more if needed
                }
                if dtype_str in mapping:
                    attr.type = onnx.AttributeProto.INT
                    attr.i = mapping[dtype_str]
                    attr.ClearField("s")
                else:
                    print(f"Unknown Cast type: {dtype_str}")

onnx.save(model, fixed_model_path)

# TF-IDF for text
vectorizer = TfidfVectorizer(max_features=5000)
X_train_text = vectorizer.fit_transform(X_train["cleaned text"])
X_test_text = vectorizer.transform(X_test["cleaned text"])

# Scale numeric features
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train_num = scaler.fit_transform(X_train[["link_count", "email length"]])
X_test_num = scaler.transform(X_test[["link_count", "email length"]])

# Combine
X_train_combined = hstack([X_train_text, X_train_num])
X_test_combined = hstack([X_test_text, X_test_num])

from sklearn.naive_bayes import MultinomialNB

nb_model = MultinomialNB()
nb_model.fit(X_train_combined, y_train)

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_combined, y_train)

comparison_dir = "/content/drive/My Drive/Cybersecurity Practicum/Custom_CV_phishing_results/Comparison_results"
os.makedirs(comparison_dir, exist_ok=True)

results = []
results.append(evaluate_classical_model(nb_model, X_test_combined, y_test, name="Naive Bayes", output_dir=comparison_dir))
results.append(evaluate_classical_model(lr_model, X_test_combined, y_test, name="Logistic Regression", output_dir=comparison_dir))

from sklearn.utils import resample
from sklearn.calibration import CalibratedClassifierCV

X_sample, y_sample = resample(X_train_combined, y_train, n_samples=2000, random_state=42)
base_svm = SVC()  # probability=False by default
svm_model = CalibratedClassifierCV(base_svm, cv=3)
svm_model.fit(X_sample, y_sample)

results += [
    evaluate_classical_model(svm_model, X_test_combined, y_test, name="SVM", output_dir=comparison_dir),
]

# Random Forest
rf_model = RandomForestClassifier(
    n_estimators=50,
    max_depth=10,
    n_jobs=-1,
    random_state=42
)
rf_model.fit(X_sample, y_sample)

results += [
    evaluate_classical_model(rf_model, X_test_combined, y_test, name="Random Forest", output_dir=comparison_dir),
]

# XGBoost
xgb_model = xgb.XGBClassifier(
    n_estimators=50,
    max_depth=4,
    subsample=0.8,
    colsample_bytree=0.8,
    use_label_encoder=False,
    eval_metric="logloss",
    n_jobs=-1
)

xgb_model.fit(X_sample, y_sample)

results += [
    evaluate_classical_model(xgb_model, X_test_combined, y_test, name="XGBoost", output_dir=comparison_dir)
]

# Imports
from transformers import (
    Trainer, TrainingArguments
)
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.model_selection import train_test_split

# Extract text only
train_texts = list(X_train["cleaned text"])
train_labels = list(y_train)  # or y_train.tolist()

test_texts = list(X_test["cleaned text"])
test_labels = list(y_test)

from transformers import AutoTokenizer, AutoModelForSequenceClassification

set_seed(42)

bert_model_name = "bert-base-uncased"
distilbert_model_name = "distilbert-base-uncased"

bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=2).to("cuda")

distilbert_tokenizer = AutoTokenizer.from_pretrained(distilbert_model_name)
distilbert_model = AutoModelForSequenceClassification.from_pretrained(distilbert_model_name, num_labels=2).to("cuda")

from sklearn.model_selection import StratifiedKFold

def save_classification_report(report_str, filename):
    with open(filename, 'w') as f:
        f.write(report_str)
    print(f"Saved classification report to {filename}")

import os
import numpy as np
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay, classification_report
)
import matplotlib.pyplot as plt
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, EarlyStoppingCallback
)
from datasets import Dataset

def run_k_fold_training(model_name, encodings, labels, k=5, output_dir="bert_kfold_results"):
    os.makedirs(output_dir, exist_ok=True)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    all_metrics = []

    for fold, (train_idx, val_idx) in enumerate(skf.split(encodings["input_ids"], labels), 1):
        fold_model_dir = os.path.join(output_dir, f"model_fold_{fold}")
        report_path = os.path.join(fold_model_dir, "classification_report.txt")

        # Skip completed folds
        if os.path.exists(report_path):
            print(f"Fold {fold} already completed — skipping.")
            continue

        print(f"\nStarting Fold {fold}/{k}")
        os.makedirs(fold_model_dir, exist_ok=True)

        # Prepare fold data
        train_dataset = Dataset.from_dict({
            "input_ids": [encodings["input_ids"][i] for i in train_idx],
            "attention_mask": [encodings["attention_mask"][i] for i in train_idx],
            "labels": [labels[i] for i in train_idx],
        })

        val_dataset = Dataset.from_dict({
            "input_ids": [encodings["input_ids"][i] for i in val_idx],
            "attention_mask": [encodings["attention_mask"][i] for i in val_idx],
            "labels": [labels[i] for i in val_idx],
        })

        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to("cuda")

        training_args = TrainingArguments(
            output_dir=os.path.join(output_dir, f"fold_{fold}"),
            evaluation_strategy="epoch",
            save_strategy="epoch",
            learning_rate=1e-5,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=16,
            num_train_epochs=3,
            weight_decay=0.01,
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            logging_dir=os.path.join(output_dir, f"logs/fold_{fold}"),
            logging_steps=10,
            save_total_limit=1,
        )

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            compute_metrics=compute_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
        )

        trainer.train()

        # Evaluate
        eval_result = trainer.evaluate()
        all_metrics.append(eval_result)
        print(f"Fold {fold} results:", eval_result)

        # Predictions
        predictions = trainer.predict(val_dataset)
        y_pred = np.argmax(predictions.predictions, axis=1)
        y_true = predictions.label_ids

        # Save confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm)
        disp.plot(cmap="Blues")
        plt.title(f"Confusion Matrix - Fold {fold}")
        cm_path = os.path.join(output_dir, f"confusion_matrix_fold_{fold}.png")
        plt.savefig(cm_path)
        plt.close()
        print(f"Confusion matrix saved to {cm_path}")

        # Save model and tokenizer
        trainer.save_model(fold_model_dir)
        tokenizer.save_pretrained(fold_model_dir)
        print(f"Model and tokenizer saved to {fold_model_dir}")

        # Save predictions and true labels
        np.save(os.path.join(fold_model_dir, "val_preds.npy"), y_pred)
        np.save(os.path.join(fold_model_dir, "val_labels.npy"), y_true)

        # Save classification report
        val_class_report = classification_report(
            y_true, y_pred, target_names=["Legit", "Phish"], digits=4
        )
        with open(report_path, "w") as f:
            f.write(val_class_report)
        print(f"Classification report saved to {report_path}")

        # Confirm what was saved
        print(f"Fold {fold} saved files:", os.listdir(fold_model_dir))

    # Average metrics only over completed folds
    if all_metrics:
        avg_metrics = {metric: np.mean([m[metric] for m in all_metrics]) for metric in all_metrics[0]}
        std_metrics = {metric: np.std([m[metric] for m in all_metrics]) for metric in all_metrics[0]}

        print("\nAverage Results Across Completed Folds:")
        for k in avg_metrics:
            print(f"{k}: {avg_metrics[k]:.4f} ± {std_metrics[k]:.4f}")
    else:
        avg_metrics = std_metrics = {}
        print("No folds were trained in this run.")

    return all_metrics, avg_metrics, std_metrics

# Tokenize
encodings = bert_tokenizer(train_texts, truncation=True, padding=True, max_length=512)

from scipy.special import softmax
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import accuracy_score, roc_auc_score
from scipy.special import softmax
import numpy as np

def compute_metrics(eval_pred):
    # eval_pred is now a `EvalPrediction` object with .predictions and .label_ids
    logits = eval_pred.predictions
    labels = eval_pred.label_ids

    # Sanitize logits
    logits = np.array(logits, dtype=np.float32)
    if not np.isfinite(logits).all():
        print("Detected NaN or Inf in logits before softmax")
        logits = np.nan_to_num(logits, nan=0.0, posinf=10.0, neginf=-10.0)

    predictions = np.argmax(logits, axis=-1)

    # Convert logits to probabilities using softmax
    if logits.shape[1] == 2:
      probs = softmax(logits, axis=1)[:, 1]  # Probability for class 1
    else:
      raise ValueError(f"Unexpected logits shape: {logits.shape}. Expected 2 classes for binary classification.")

    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='binary', zero_division=0
    )
    acc = accuracy_score(labels, predictions)

    try:
        roc_auc = roc_auc_score(labels, probs)
    except ValueError:
        roc_auc = float('nan')

    return {
        "accuracy": acc,
        "f1": f1,
        "precision": precision,
        "recall": recall,
        "roc_auc": roc_auc,
    }

from transformers import EarlyStoppingCallback

# Run
metrics, avg, std = run_k_fold_training(bert_model_name, encodings, train_labels, k=5, output_dir=os.path.join(output_dir, "new_bert_kfold_results"))

print("Training finished. Verifying model fold outputs:")
!ls -lhR "/content/drive/My Drive/Cybersecurity Practicum/Custom_CV_phishing_results/Performance_Comparison/new_bert_kfold_results/"

best_model_dir = os.path.join(output_dir, 'new_bert_kfold_results', f"model_fold_3") #TODO Change this to the actual best one
bert_model = AutoModelForSequenceClassification.from_pretrained(best_model_dir)
print(f"Best model loaded from {best_model_dir}")

results = []
import seaborn as sns

# BERT text-only
results.append(evaluate_text_only_model(
    model=bert_model,
    tokenizer=bert_tokenizer,
    samples=test_texts,
    labels=test_labels,
    batch_size=32,
    device="cuda",
    name="BERT",
    output_dir=output_dir
))

# Tokenize
encodings = distilbert_tokenizer(train_texts, truncation=True, padding=True, max_length=512)

# Run
metrics, avg, std = run_k_fold_training(distilbert_model_name, encodings, train_labels, k=5, output_dir=os.path.join(output_dir, "distilbert_kfold_results"))

best_model_dir = os.path.join(output_dir, 'distilbert_kfold_results', f"model_fold_2") #Set this to the best one after reviewing the metrics
distilbert_model = AutoModelForSequenceClassification.from_pretrained(best_model_dir)
print(f"Best model loaded from {best_model_dir}")

# DistilBERT text-only
results.append(evaluate_text_only_model(
    model=distilbert_model,
    tokenizer=distilbert_tokenizer,
    samples=test_texts,
    labels=test_labels,
    batch_size=32,
    device="cuda",
    name="DistilBERT",
    output_dir=output_dir
))

import pandas as pd
df_results = pd.DataFrame(results)
df_results.sort_values(by="f1", ascending=False, inplace=True)
display(df_results)

df_results.to_csv(os.path.join(comparison_dir, "model_comparison.csv"), index=False)

import matplotlib.pyplot as plt
import seaborn as sns

# Set visual style
sns.set(style="whitegrid")

def plot_model_scores(df_results):
    melted = df_results.melt(
        id_vars="name",
        value_vars=["accuracy", "f1"],
        var_name="Metric",
        value_name="Score"
    )

    plt.figure(figsize=(10, 6))
    ax = sns.barplot(x="name", y="Score", hue="Metric", data=melted)
    plt.title("Model Accuracy & F1 Score Comparison")
    plt.xticks(rotation=45, ha="right")
    plt.ylim(0, 1.05)
    for container in ax.containers:
        ax.bar_label(container, fmt="%.2f", label_type="edge", padding=2)
    plt.tight_layout()
    plt.show()
    plt.savefig(os.path.join(comparison_dir, "model_scores.png"))

def plot_inference_times(df_results):
    plt.figure(figsize=(10, 5))
    ax = sns.barplot(x="name", y="inference_time", data=df_results, palette="crest")
    plt.title("Model Inference Time (Batch)")
    plt.ylabel("Seconds")
    plt.xticks(rotation=45, ha="right")
    for container in ax.containers:
        ax.bar_label(container, fmt="%.4f", label_type="edge", padding=2)
    plt.tight_layout()
    plt.show()
    plt.savefig(os.path.join(comparison_dir, "inference_times.png"))

plot_model_scores(df_results)
plot_inference_times(df_results)